from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn.functional as F
import os

## KoGPT2 ê³µê° ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ë° ì„¤ì • ì ìš©
MODEL_NAME = "dlckdfuf141/empathy-kogpt2"
MAX_NEW_TOKENS = 40 
TEMPERATURE = 0.7
TOP_P = 0.95

## ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ë¡œë“œ)
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    ## CPU/GPU ìë™ ê°ì§€ ë° ë¡œë“œ (cuda ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•˜ë©´ cpuë¡œ ëŒ€ì²´)
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ## KoGPT2ëŠ” pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ, eos_tokenì„ pad_tokenìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
    tokenizer.pad_token = tokenizer.eos_token 
    
    ## ëª¨ë¸ì„ DEVICEì— ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)
    print(f"INFO: KoGPT-2 Empathy model loaded successfully on {DEVICE}.")
except Exception as e:
    print(f"ERROR: Failed to load KoGPT-2 Empathy model: {e}")
    ## ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ë”ë¯¸ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ì—¬ ì„œë²„ëŠ” ê³„ì† ì‘ë™í•˜ë„ë¡ í•¨
    def tokenizer(*args, **kwargs): return {}
    def model(*args, **kwargs): pass
    DEVICE = "cpu"

def generate_comment(content: str, emotion_label: str) -> str:

    
    # ğŸŒŸ í˜ë¥´ì†Œë‚˜ ë° ì§€ì‹œì‚¬í•­ ì¶”ê°€ (í”„ë¡¬í”„íŠ¸ êµ¬ì„±)
    system_instruction = (
        f"ë‹¹ì‹ ì€ ë”°ëœ»í•œ ì‹¬ë¦¬ ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê°ì •({emotion_label})ê³¼ ì¼ê¸° ë‚´ìš©ì„ ê³µê°í•˜ì—¬ ììƒí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì¼ê¸° ë‚´ìš©: {content}\n"
        f"ê³µê° ë©”ì‹œì§€:"
    )
    
    # KoGPT2 ëª¨ë¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹: [ì§€ì‹œì‚¬í•­]\n\nê°ì •: {emotion_label}\nì¼ê¸°: {content}\nê³µê° ë©”ì‹œì§€:
    prompt = f"{system_instruction}\n\nê°ì •: {emotion_label}\nì¼ê¸°: {content}\nê³µê° ë©”ì‹œì§€:"
    
    try:
        # return_tensors="pt" ì‚¬ìš©
        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

        ## ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                top_p=TOP_P,
                temperature=TEMPERATURE,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_return_sequences=1
            )
        
        # ê²°ê³¼ ë””ì½”ë”© ë° ì‘ë‹µ íŒŒì‹±
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # "ê³µê° ë©”ì‹œì§€:" ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        response = generated_text.split("ê³µê° ë©”ì‹œì§€:")[-1].strip()
        
        # ë¶ˆí•„ìš”í•œ í”„ë¡¬í”„íŠ¸ ì”ì—¬ë¬¼ ì œê±° ë° ê¸¸ì´ ì œí•œ ì ìš©
        if "\n" in response:
            response = response.split("\n")[0].strip()
        if "ì¼ê¸° ë‚´ìš©:" in response:
            response = response.split("ì¼ê¸° ë‚´ìš©:")[0].strip()
            
        # ê¸¸ì´ ì œí•œ ì ìš©
        if len(response) > 50:
            return response[:50].strip() + "..."
        
        return response

    except Exception as e:
        print(f"ERROR during comment generation: {e}")
        # AI ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ë¼ìš°í„°ì—ì„œ ì„¤ì •í•œ ê¸°ë³¸ ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´
        raise Exception(f"AI ì½”ë©˜íŠ¸ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")