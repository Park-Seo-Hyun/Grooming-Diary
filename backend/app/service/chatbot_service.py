from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn.functional as F
import os
from typing import Dict, Any

## âš ï¸ FINAL FIX: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ë¡œì»¬ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
LOCAL_MODEL_PATH = r"D:/Grooming/backend/app/ai_model/fine_tuned_kogpt2_model" 

# ğŸŒŸ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ 'Tender'ë¥¼ 'í–‰ë³µ'ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ìˆ˜ì •í•¨ ğŸŒŸ
EMOTION_ENG_TO_KOR: Dict[str, str] = {
    "Angry": "ë¶„ë…¸",
    "Fear": "ê³µí¬",
    "Sad": "ìŠ¬í””",
    "Happy": "í–‰ë³µ",
    "Tender": "í–‰ë³µ",
    "Neutral": "ì¤‘ë¦½"
}

# -------------------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ --------------------------
LOAD_SUCCESS = False
try:
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)

    DEVICE = torch.device("cpu")
    tokenizer.pad_token = tokenizer.eos_token 
    
    model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_PATH).to(DEVICE)
    model.eval()
    
    print(f"INFO: KoGPT-2 Empathy model loaded successfully from local path on {DEVICE}.")
    LOAD_SUCCESS = True

except Exception as e:
    print(f"ERROR: Failed to load KoGPT-2 Empathy model from {LOCAL_MODEL_PATH}. Error: {e}")

    def tokenizer(*args, **kwargs):
        try:
            temp_tokenizer = AutoTokenizer.from_pretrained("dlckdfuf141/empathy-kogpt2")
            return temp_tokenizer(*args, **kwargs)
        except Exception:
            return {'input_ids': torch.tensor([[101, 102]]), 'attention_mask': torch.tensor([[1, 1]])}
    
    class DummyModel:
        def generate(self, input_ids, **kwargs):
            dummy_text = "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì£„ì†¡í•©ë‹ˆë‹¤."
            try:
                temp_tokenizer = AutoTokenizer.from_pretrained("dlckdfuf141/empathy-kogpt2")
                dummy_output = temp_tokenizer(dummy_text, return_tensors="pt").input_ids
                return dummy_output
            except Exception:
                return torch.tensor([[101, 102]])

        def to(self, device): return self
        def eval(self): pass

    model = DummyModel()
    DEVICE = "cpu"
    LOAD_SUCCESS = False


def generate_comment(content: str, emotion_label: str) -> str:
    if not LOAD_SUCCESS:
        return "í˜„ì¬ AI ì±—ë´‡ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        
    korean_emotion = EMOTION_ENG_TO_KOR.get(emotion_label, "ì¤‘ë¦½")
    
    prompt = (
        f"ê°ì •: {korean_emotion}\n"
        f"ì¼ê¸°: {content}\n"
        f"ê³µê° ë©”ì‹œì§€: "
    )
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
        
        MAX_NEW_TOKENS = 100
        TEMPERATURE = 0.7
        TOP_P = 0.95
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                top_p=TOP_P,
                temperature=TEMPERATURE,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_return_sequences=1
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # -------------------------------------------------------
        # ğŸŒŸ FIX HERE: "ì¹œêµ¬ì˜ ë°˜ì‘:" â†’ "ê³µê° ë©”ì‹œì§€:" ë¡œ ë³€ê²½
        # -------------------------------------------------------
        response = generated_text.split("ê³µê° ë©”ì‹œì§€:")[-1].strip()
        # -------------------------------------------------------

        # ë¶ˆí•„ìš”í•œ í”„ë¡¬í”„íŠ¸ ì”ì—¬ë¬¼ ì œê±°
        if "ì¼ê¸° ë‚´ìš©:" in response:
            response = response.split("ì¼ê¸° ë‚´ìš©:")[0].strip()
            
        if "ì‚¬ìš©ìë‹˜ì˜ ë‹µë³€ì„ ë“£ê³  ì–´ë–¤" in response:
            return "ì—ì´, ê¸°ìš´ ë‚´ ì¹œêµ¬ì•¼! ë„ˆì˜ ì´ì•¼ê¸°ë¥¼ ë“¤ì–´ì¤„ê²Œ."
            
        if "\n" in response:
            response = response.split("\n")[0].strip()
        
        if len(response) > 197:
            return response[:197].strip() + "..."
            
        return response

    except Exception as e:
        print(f"ERROR during comment generation: {e}")
        return "ì•¼, ë¯¸ì•ˆ! ì§€ê¸ˆ AI ì¹œêµ¬ê°€ ì ê¹ ì •ì‹ ì„ ë†¨ì–´. ë‹¤ì‹œ í•œë²ˆ ì‹œë„í•´ë³¼ê²Œ."
