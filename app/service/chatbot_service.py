from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn.functional as F
import os

## KoGPT2 ê³µê° ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ë° ì„¤ì • ì ìš©
MODEL_NAME = "dlckdfuf141/empathy-kogpt2"
MAX_NEW_TOKENS = 60 
TEMPERATURE = 0.8
TOP_P = 0.95

## ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ë¡œë“œ)
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    ## CPU/GPU ìë™ ê°ì§€ ë° ë¡œë“œ (cuda ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•˜ë©´ cpuë¡œ ëŒ€ì²´)
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ## KoGPT2ëŠ” pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ, eos_tokenì„ pad_tokenìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
    tokenizer.pad_token = tokenizer.eos_token 
    
    ## ëª¨ë¸ì„ DEVICEì— ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)
    print(f"INFO: KoGPT-2 Empathy model loaded successfully on {DEVICE}.")
except Exception as e:
    print(f"ERROR: Failed to load KoGPT-2 Empathy model: {e}")
    ## ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ë”ë¯¸ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ì—¬ ì„œë²„ëŠ” ê³„ì† ì‘ë™í•˜ë„ë¡ í•¨
    def tokenizer(*args, **kwargs): return {}
    def model(*args, **kwargs): pass
    DEVICE = "cpu"

def generate_comment(content: str, emotion_label: str) -> str:
    """
    ì‚¬ìš©ìì˜ ì¼ê¸° ë‚´ìš©ê³¼ ê°ì • ë ˆì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ ê³µê° ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    (ë”°ëœ»í•œ ì‹¬ë¦¬ ìƒë‹´ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ ë¶€ì—¬)
    """
    
    # ğŸŒŸ í˜ë¥´ì†Œë‚˜ ë° ì§€ì‹œì‚¬í•­ ì¶”ê°€ (í”„ë¡¬í”„íŠ¸ êµ¬ì„±)
    system_instruction = (
        "ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì‹¬ë¦¬ ìƒë‹´ê°€ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ì¼ê¸°ë¥¼ ë³´ê³ , 50ì ì´ë‚´ì˜ ì§§ê³  ê²©ë ¤ê°€ ë˜ëŠ” ì¹œêµ¬ê°€ ë³´ë‚´ëŠ” ë“¯í•œ ê³µê° ë©”ì‹œì§€ë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."
    )
    
    # KoGPT2 ëª¨ë¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹: [ì§€ì‹œì‚¬í•­]\n\nê°ì •: {emotion_label}\nì¼ê¸°: {content}\nê³µê° ë©”ì‹œì§€:
    prompt = f"{system_instruction}\n\nê°ì •: {emotion_label}\nì¼ê¸°: {content}\nê³µê° ë©”ì‹œì§€:"
    
    try:
        # return_tensors="pt" ì‚¬ìš©
        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

        ## ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                top_p=TOP_P,
                temperature=TEMPERATURE,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_return_sequences=1
            )
        
        # ê²°ê³¼ ë””ì½”ë”© ë° ì‘ë‹µ íŒŒì‹±
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # "ê³µê° ë©”ì‹œì§€:" ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        response = generated_text.split("ê³µê° ë©”ì‹œì§€:")[-1].strip()
        
        # ë¶ˆí•„ìš”í•œ í”„ë¡¬í”„íŠ¸ ì”ì—¬ë¬¼ ì œê±° ë° ê¸¸ì´ ì œí•œ ì ìš©
        if len(response) > 50:
            return response[:50].strip() + "..."
        
        return response

    except Exception as e:
        print(f"ERROR during comment generation: {e}")
        # AI ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ë¼ìš°í„°ì—ì„œ ì„¤ì •í•œ ê¸°ë³¸ ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´
        raise Exception(f"AI ì½”ë©˜íŠ¸ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")