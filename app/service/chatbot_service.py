from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn.functional as F
import os
from typing import Dict, Any

## âš ï¸ FINAL FIX: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ë¡œì»¬ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
# ì´ ê²½ë¡œë¥¼ Google Drive ë™ê¸°í™”ëœ ëª¨ë¸ í´ë”ì˜ ì‹¤ì œ ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆì‹œ: D:/MyDrive/Grooming/backend/ai_model/fine_tuned_kogpt2_model
LOCAL_MODEL_PATH = "D:/Grooming/backend/ai_model/fine_tuned_kogpt2_model" 

# ğŸŒŸ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ 'Tender'ë¥¼ 'í–‰ë³µ'ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ìˆ˜ì •í•¨ ğŸŒŸ
EMOTION_ENG_TO_KOR: Dict[str, str] = {
    "Angry": "ë¶„ë…¸",
    "Fear": "ê³µí¬",
    "Sad": "ìŠ¬í””",
    "Happy": "í–‰ë³µ",
    "Tender": "í–‰ë³µ",  # ìš”ì²­ì— ë”°ë¼ 'ë¶€ë“œëŸ¬ì›€' ëŒ€ì‹  'í–‰ë³µ'ìœ¼ë¡œ ì„¤ì •
    "Neutral": "ì¤‘ë¦½"
}

# -------------------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ë¡œë“œ) --------------------------
LOAD_SUCCESS = False
try:
    # âš ï¸ FIX: ëª¨ë¸ ë¡œë“œ ê²½ë¡œë¥¼ ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ê²½ë¡œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)
    
    # âš ï¸ CPUë¡œ ê°•ì œ ì§€ì • (ë…¸íŠ¸ë¶ í™˜ê²½ ì•ˆì •í™”)
    DEVICE = torch.device("cpu")
    
    tokenizer.pad_token = tokenizer.eos_token 
    
    ## ëª¨ë¸ì„ DEVICEì— ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_PATH).to(DEVICE)
    model.eval() # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
    
    print(f"INFO: KoGPT-2 Empathy model loaded successfully from local path on {DEVICE}.")
    LOAD_SUCCESS = True
except Exception as e:
    print(f"ERROR: Failed to load KoGPT-2 Empathy model from {LOCAL_MODEL_PATH}. Error: {e}")
    # ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ë”ë¯¸ í•¨ìˆ˜ë¡œ ëŒ€ì²´í•˜ì—¬ ì„œë²„ëŠ” ê³„ì† ì‘ë™í•˜ë„ë¡ í•¨
    def tokenizer(*args, **kwargs): 
        # ë”ë¯¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ BASE_MODEL_ID í† í¬ë‚˜ì´ì €ë¥¼ ì„ì‹œ ë¡œë“œ (í•„ìˆ˜)
        try:
            temp_tokenizer = AutoTokenizer.from_pretrained("dlckdfuf141/empathy-kogpt2")
            return temp_tokenizer(*args, **kwargs)
        except Exception:
            return {'input_ids': torch.tensor([[101, 102]]), 'attention_mask': torch.tensor([[1, 1]])}
    
    class DummyModel:
        def generate(self, input_ids, **kwargs):
            dummy_text = "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì£„ì†¡í•©ë‹ˆë‹¤."
            try:
                temp_tokenizer = AutoTokenizer.from_pretrained("dlckdfuf141/empathy-kogpt2")
                dummy_output = temp_tokenizer(dummy_text, return_tensors="pt").input_ids
                return dummy_output
            except Exception:
                return torch.tensor([[101, 102]])

        def to(self, device): return self
        def eval(self): pass

    model = DummyModel()
    DEVICE = "cpu"
    LOAD_SUCCESS = False


def generate_comment(content: str, emotion_label: str) -> str:
    """
    ì‚¬ìš©ìì˜ ì¼ê¸° ë‚´ìš©ê³¼ ê°ì • ë ˆì´ë¸”ì„ ë°”íƒ•ìœ¼ë¡œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ê³µê° ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì¹œí•œ ì¹œêµ¬ì²˜ëŸ¼ ë°˜ë§ì„ ì‚¬ìš©í•˜ë©°, ì‘ë‹µ í’ˆì§ˆì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
    """
    # âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ, ê¸°ë³¸ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•˜ì—¬ ì„œë²„ ë‹¤ìš´ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    if not LOAD_SUCCESS:
        return "í˜„ì¬ AI ì±—ë´‡ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        
    korean_emotion = EMOTION_ENG_TO_KOR.get(emotion_label, "ì¤‘ë¦½")
    
    # ğŸŒŸ FIX: ë°˜ë³µ/ì”ì—¬ë¬¼ ì¶œë ¥ì„ ë§‰ê¸° ìœ„í•´ í”„ë¡¬í”„íŠ¸ì— ê¸ˆì§€ ë‹¨ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
    prompt = (
        f"ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì´ì•¼ê¸°ë¥¼ ê²½ì²­í•˜ê³  ê³µê°í•˜ëŠ” ë”°ëœ»í•˜ê³  ì •ì¤‘í•œ ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì§€ê¸ˆë¶€í„° ì‚¬ìš©ìì—ê²Œ **ì¡´ëŒ“ë§(í•˜ì‹­ì‹œì˜¤ì²´)**ë§Œì„ ì‚¬ìš©í•´ ë‹µë³€í•˜ì‹­ì‹œì˜¤. "
        f"ì‚¬ìš©ìì˜ **{korean_emotion} ê°ì •ë§Œì„ ì–¸ê¸‰í•˜ë©°** ì¼ê¸° ë‚´ìš©ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ , í•´ë‹¹ ê°ì •ì— ë§ì¶”ì–´ ì •ì¤‘í•˜ê²Œ ìœ„ë¡œì™€ ì¡°ì–¸ì„ ë“œë¦½ë‹ˆë‹¤. " # FIX: ê°ì • ì–¸ê¸‰ ì§€ì‹œ ê°•í™”
        f"ì‘ë‹µì€ ì¼ê¸° ë‚´ìš©ì—ë§Œ ê´€ë ¨ë˜ê²Œ ì‘ì„±í•˜ê³ , 'ì–´ë–¤:', 'ë°˜ì‘ì€', 'í‹±ìœ¼ë¡œ', 'ë¶„ë…¸', 'ìŠ¬í””', 'ê³µí¬' ë“±ì˜ ë‹¤ë¥¸ ê°ì • ë‹¨ì–´ëŠ” ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n\n" # FIX: ê¸ˆì§€ ë‹¨ì–´ ëª©ë¡ì— ë‹¤ë¥¸ ë¶€ì • ê°ì • ì¶”ê°€
        f"ì¼ê¸° ë‚´ìš©: {content}\n"
        f"ìƒë‹´ê°€ì˜ ì‘ë‹µ:" 
    )
    
    try:
        # return_tensors="pt" ì‚¬ìš©
        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
        
        # âš ï¸ FIX: Max Token ê°’ì„ 50ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ ë¬¸ì¥ ì™„ì„±ë„ë¥¼ ë†’ì„. (ì‚¬ìš©ì ìš”ì²­: ìµœëŒ€ 200ì ëª©í‘œ)
        MAX_NEW_TOKENS = 100 # 200ì ìƒì„±ì„ ìœ„í•´ í† í° ìˆ˜ë¥¼ 100ìœ¼ë¡œ ì¦ê°€
        TEMPERATURE = 0.7
        TOP_P = 0.95
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                # ğŸŒŸ FIX: attention_maskë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ê²½ê³  í•´ê²°
                attention_mask=inputs.attention_mask,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                top_p=TOP_P,
                temperature=TEMPERATURE,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_return_sequences=1
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ğŸŒŸ FIX: ì‘ë‹µ íŒŒì‹±ì„ 'ì¹œêµ¬ì˜ ë°˜ì‘:' ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ë„ë¡ ìˆ˜ì •
        response = generated_text.split("ì¹œêµ¬ì˜ ë°˜ì‘:")[-1].strip()
        
        # ë¶ˆí•„ìš”í•œ í”„ë¡¬í”„íŠ¸ ì”ì—¬ë¬¼ ë° ëª¨ë¸ì´ ë°˜ë³µí•œ ë‚´ìš© ì œê±° í›„ì²˜ë¦¬
        # 1. ì¼ê¸° ë‚´ìš©ì´ ë‹¤ì‹œ ë‚˜íƒ€ë‚˜ë©´ ê·¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        if "ì¼ê¸° ë‚´ìš©:" in response:
            response = response.split("ì¼ê¸° ë‚´ìš©:")[0].strip()
            
        # 2. ì”ì—¬ í”„ë¡¬í”„íŠ¸ ì œê±° (ì´ì „ ì˜¤ë¥˜ íŒ¨í„´ í¬í•¨)
        if "ì‚¬ìš©ìë‹˜ì˜ ë‹µë³€ì„ ë“£ê³  ì–´ë–¤" in response:
            return "ì—ì´, ê¸°ìš´ ë‚´ ì¹œêµ¬ì•¼! ë„ˆì˜ ì´ì•¼ê¸°ë¥¼ ë“¤ì–´ì¤„ê²Œ."
            
        # 3. ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ì¸í•´ ë¬¸ì¥ì´ ëŠê¸°ëŠ” ê²½ìš° ì²« ì¤„ë§Œ ì‚¬ìš©
        if "\n" in response:
            response = response.split("\n")[0].strip()
        
        # 4. ê¸¸ì´ ì œí•œ ì ìš© (200ì ì´ˆê³¼ ì‹œ ... ì¶”ê°€)
        if len(response) > 200:
            return response[:200].strip() + "..."
            
        return response

    except Exception as e:
        print(f"ERROR during comment generation: {e}")
        # AI ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ê¸°ë³¸ ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
        return "ì•¼, ë¯¸ì•ˆ! ì§€ê¸ˆ AI ì¹œêµ¬ê°€ ì ê¹ ì •ì‹ ì„ ë†¨ì–´. ë‹¤ì‹œ í•œë²ˆ ì‹œë„í•´ë³¼ê²Œ." # ì¹œêµ¬ ì»¨ì…‰ì— ë§ê²Œ ë³€ê²½